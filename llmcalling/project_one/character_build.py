"""
é¡¹ç›®1ï¼šæ„å»ºä¸€ä¸ªå¤šåŠŸèƒ½è§’è‰²èŠå¤©æœºå™¨äºº

ç›®æ ‡: ç†Ÿç»ƒä½¿ç”¨ OpenAI APIå’Œ Prompt Engineering
ä»»åŠ¡: åˆ›å»ºä¸€ä¸ªç®€å•çš„ Web ç•Œé¢ï¼ˆç”¨ Streamlit ï¼‰ï¼Œ
ç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸åŒçš„è§’è‰²ï¼ˆå¦‚â€œPythonç¼–ç¨‹åŠ©æ‰‹â€ã€â€œè‹±è¯­å£è¯­æ•™ç»ƒâ€ã€â€œè‹æ ¼æ‹‰åº•â€ï¼‰ï¼Œ
ç¨‹åºä¼šæ ¹æ®é€‰æ‹©ï¼Œä½¿ç”¨ä¸åŒçš„System Promptä¸ç”¨æˆ·è¿›è¡Œé«˜è´¨é‡å¯¹è¯ã€‚
æ”¶è·: ç²¾é€šAPIè°ƒç”¨ã€æŒæ¡æ ¸å¿ƒçš„æç¤ºå·¥ç¨‹æŠ€å·§

run:
streamlit run llmcalling/project_one/character_build.py
"""

import streamlit as st
import os
from openai import OpenAI
from dotenv import load_dotenv
from prompt_infactor import system_prompts

load_dotenv()

st.set_page_config(page_title="AI Character Building", page_icon="ğŸ¥°")

client = OpenAI(
    api_key=os.environ.get("ALIYUN_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# Sidebar building
with st.sidebar:
    st.header("ğŸ­ Choose a character")
    selected_role = st.selectbox("Please choose an AI chatbot: ", list(system_prompts.keys()))

    system_prompt = system_prompts[selected_role]

    if st.button("Clear chat history"):
        st.session_state.messages = []
        st.rerun()

    st.divider()
    st.write(f"Current Prompt: \n{system_prompt}")


# state management
# if messages not in statement, initialize it
if "messages" not in st.session_state:
    st.session_state.messages = []

# Character change logic
if "current_role" not in st.session_state:
    st.session_state.current_role = selected_role
elif st.session_state.current_role != selected_role:
    # current role is not selected role => chat character has changed
    # clear chat history
    st.session_state.messages = []
    st.session_state.current_role = selected_role

# Render chat history and display in the box
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if "reasoning" in msg and msg["reasoning"]:
            with st.status("Finish thinking", state="complete", expanded=False):
                st.markdown(msg["reasoning"])
        st.write(msg["content"])

# Controller: process user input
if user_input := st.chat_input("Chat with it..."):
    # 1. show user input
    with st.chat_message("User:"):
        st.write(user_input)

    st.session_state.messages.append({"role": "user", "content": user_input})

    # 2. build API request with complete message: [system prompt] + [history]
    api_messages = [
        {"role": "system", "content": system_prompt},
    ] + st.session_state.messages

    # 3. get stream responses
    with st.chat_message("assistant"):
        status_box = st.status("ğŸ¤” AI æ­£åœ¨æ·±åº¦æ€è€ƒ...", expanded=True)
        with status_box:
            reasoning_placeholder = st.empty()
        content_placeholder = st.empty()

        responses = client.chat.completions.create(
            model="qwen3-vl-32b-thinking",
            messages=api_messages,
            temperature=0.6,
            # æ³¨æ„ï¼šåªæœ‰ç‰¹å®šæ¨¡å‹æ”¯æŒ extra_body å‚æ•°ï¼Œå¦‚æœæŠ¥é”™è¯·ç¡®è®¤æ¨¡å‹æ–‡æ¡£
            extra_body={
                "enable_thinking": True,
                "thinking_budget": 500,
            },
            stream=True,
            stream_options={"include_usage": True}
        )

        # 4. process returned chunks
        reasoning_chunks = []
        content_chunks = []
        for chunk in responses:
            # process message contain token used which is the last chunk
            if not chunk.choices:
                usage = f"Consume Token: {chunk.usage.total_tokens}"
                status_box.update(label="Thinking end.  " + usage, state="complete", expanded=False)
                print(usage, end='', flush=True)
            else:
                delta = chunk.choices[0].delta
                # process reasoning content
                access_chunk = getattr(delta, "reasoning_content", None)
                if access_chunk:
                    print(access_chunk, end='', flush=True)
                    reasoning_chunks.append(access_chunk)
                    # refresh UI in real time
                    reasoning_placeholder.markdown("".join(reasoning_chunks))

                # process response content
                if delta.content:
                    print(delta.content, end='', flush=True)
                    content_chunks.append(delta.content)
                    content_placeholder.markdown("".join(content_chunks) + "â–Œ")

        full_reasoning = "".join(reasoning_chunks)
        full_content = "".join(content_chunks)

        # remove "â–Œ" and show complete response content
        content_placeholder.markdown(full_content)

        st.session_state.messages.append({
            "role": "assistant",
            "content": full_content,
            "reasoning": full_reasoning,
        })

